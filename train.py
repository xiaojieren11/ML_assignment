import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import os
from models.LSTM import LSTMModel  # å¯¼å…¥ LSTM æ¨¡å‹
from models.Transformer import TransformerModel  # å¯¼å…¥ Transformer æ¨¡å‹
from config import get_parser  # å¯¼å…¥ Config ç±»
import datetime
import untils
from torch.utils.tensorboard import SummaryWriter

def sliding_window(dataset, time_steps=1, predict_future=False):
    X, y = [], []
    if predict_future:
        for i in range(len(dataset) - time_steps):
            X.append(dataset[i:(i + time_steps)])
            y.append(dataset[i + time_steps, 0])
    else:
        for i in range(time_steps, len(dataset)):
            X.append(dataset[i-time_steps:i])
            y.append(dataset[i, 0])
    return np.array(X), np.array(y)


def main(args):
    # 1. æ•°æ®å‡†å¤‡
    # 1.1 è¯»å–æ•°æ®
    train_data = pd.read_csv('./dataset/train_processed.csv', index_col='DateTime', parse_dates=True)
    test_data = pd.read_csv('./dataset/test_processed.csv', index_col='DateTime', parse_dates=True)

    # 1.2 å®šä¹‰ç‰¹å¾åˆ—
    features = ['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3','Sub_metering_remainder', 'RR', 'NBJRR1', 'NBJRR5', 'NBJRR10', 'NBJBROU']
    # 1.3 æ•°æ®ç¼©æ”¾
    scaler = MinMaxScaler()

    # 1.4 åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_size = args.train_size  # è®­ç»ƒé›†å–æœ€å90å¤©
    train_data = train_data[-train_size:]

    test_size = args.predict_days
    test_data = test_data[:test_size]

    train_scaled = scaler.fit_transform(train_data[features])
    test_scaled = scaler.transform(test_data[features])

    TIME_STEPS = args.time_steps
    X_train, y_train = sliding_window(train_scaled, TIME_STEPS)
    # åˆ›å»ºæµ‹è¯•é›†æ—¶ï¼Œä¸è¿›è¡Œæ»‘åŠ¨çª—å£ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®ç”¨äºé¢„æµ‹
    X_test, y_test = sliding_window(test_scaled, TIME_STEPS, predict_future=True)

    # 1.6 è½¬æ¢ä¸º PyTorch å¼ é‡
    X_train = torch.tensor(X_train, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)
    X_test = torch.tensor(X_test, dtype=torch.float32)
    y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)

    # 1.7 åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = args.batch_size
    train_dataset = TensorDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # 2. æ¨¡å‹æ„å»º
    input_size = X_train.shape[2]
    hidden_size = args.hidden_size
    output_size = args.output_size

    if args.model == 'LSTM':
        model = LSTMModel(input_size, hidden_size, output_size)
    elif args.model == 'Transformer':
        embed_dim = args.embed_dim
        dense_dim = args.dense_dim
        num_heads = args.num_heads
        model = TransformerModel(input_size, embed_dim, dense_dim, num_heads, output_size)
    else:
        raise ValueError("Invalid model type. Choose 'LSTM' or 'Transformer'.")

    # 7. ç»“æœä¿å­˜
    output_dir = os.path.join('./output', f'{args.model.lower()}_results')
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # åˆ›å»ºlogger
    logger = untils.create_logger(output_dir)

    # åˆ›å»º TensorBoard writer
    writer = SummaryWriter(log_dir=os.path.join(output_dir, 'runs'))

    # 4. æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹
    # 4.0 å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)

    # åŠ è½½æ¨¡å‹
    if args.eval_only:
        model.load_state_dict(torch.load(args.model_path,weights_only=True))
        print("å·²åŠ è½½æ¨¡å‹ï¼")
    else:
        # 4.1 æ¨¡å‹è®­ç»ƒ
        num_epochs = args.num_epochs
        num_steps = len(train_loader)
        model.train()
        start_time = datetime.datetime.now()
        global_step = 0
        for epoch in range(num_epochs):
            for idx, (X_batch, y_batch) in enumerate(train_loader):
                optimizer.zero_grad()
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                loss.backward()
                optimizer.step()

                # è®°å½•è®­ç»ƒæ—¥å¿—
                now = datetime.datetime.now()
                time_diff = now - start_time
                etas = time_diff.total_seconds() / (epoch * num_steps + idx + 1) * (num_epochs * num_steps - epoch * num_steps - idx - 1)
                print(
                    f'Train: [{epoch+1}/{num_epochs}][{idx+1}/{num_steps}]\t'
                    f'eta {datetime.timedelta(seconds=int(etas))}\t'
                    f'loss {loss.item():.4f}\t'
                )
                logger.info(
                    f'Train: [{epoch+1}/{num_epochs}][{idx+1}/{num_steps}]\t'
                    f'eta {datetime.timedelta(seconds=int(etas))}\t'
                    f'loss {loss.item():.4f}\t'
                )

                # å†™å…¥ TensorBoard
                writer.add_scalar('Loss/train', loss.item(), global_step)
                global_step += 1

        writer.close()

        # 8. ä¿å­˜æ¨¡å‹
        weight_dir = './weight'
        if not os.path.exists(weight_dir):
            os.makedirs(weight_dir)
        torch.save(model.state_dict(), os.path.join(weight_dir, f'{args.model.lower()}_model.pth'))

    # 4.3 æ¨¡å‹é¢„æµ‹
    # model.eval()
    # with torch.no_grad():
    #     predictions = []
    #     input_seq = torch.tensor(test_scaled[:TIME_STEPS], dtype=torch.float32).unsqueeze(0)
    #     num_features = input_seq.shape[2]
    #     TIME_STEPS = input_seq.shape[1]
    #     total_steps = len(test_scaled)
    #     steps = 0
    #     while len(predictions) < total_steps:
    #         output = model(input_seq)
    #         predictions.append(output.item())
    #         if len(predictions) == total_steps:
    #             break
    #         # æ„é€ æ–°è¾“å…¥ï¼š[1, 1, num_features]
    #         if steps + TIME_STEPS < total_steps:
    #             # ç”¨çœŸå®ç‰¹å¾
    #             next_input = torch.tensor(test_scaled[steps + TIME_STEPS], dtype=torch.float32).reshape(1, 1, num_features)
    #         else:
    #             # ç”¨æœ€åä¸€ä¸ªçœŸå®ç‰¹å¾å¡«å……
    #             next_input = torch.tensor(test_scaled[-1], dtype=torch.float32).reshape(1, 1, num_features)
    #         # ç”¨é¢„æµ‹å€¼æ›¿æ¢ç¬¬ä¸€ä¸ªç‰¹å¾
    #         next_input[0, 0, 0] = output.item()
    #         input_seq = torch.cat((input_seq, next_input), dim=1)[:, -TIME_STEPS:, :]
    #         steps += 1
    #     predictions = np.array(predictions)

    # 4.3 æ¨¡å‹é¢„æµ‹ â€”â€” ç›´æ¥ç”¨æ»‘åŠ¨çª—å£æ‰¹é‡é¢„æµ‹
    model.eval()
    with torch.no_grad():
        # X_test å·²ç»æ˜¯ (N, TIME_STEPS, num_features)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)  # ğŸ‘‰ è¿™é‡Œç”¨ sliding_window å¾—åˆ°çš„ X_test
        preds_scaled = model(X_test_tensor).squeeze(-1).cpu().numpy()  # (N,)

    # 5.1 é€†ç¼©æ”¾
    # å› ä¸º scaler.fit_transform æ˜¯å¯¹æ‰€æœ‰ feature åšçš„ï¼Œè¿™é‡Œæˆ‘ä»¬åªé¢„æµ‹ç¬¬ 0 åˆ—ï¼Œå…¶å®ƒåˆ—å¡« 0
    dummy = np.zeros((len(preds_scaled), X_train.shape[2] - 1))
    pred_full = np.concatenate([preds_scaled.reshape(-1, 1), dummy], axis=1)
    predictions = scaler.inverse_transform(pred_full)[:, 0]

    # çœŸå€¼ä¹Ÿè¦å¯¹é½ï¼šæ»‘çª—å y_test å¯¹åº”çš„æ˜¯ test_data ä» TIME_STEPS å¼€å§‹çš„éƒ¨åˆ†
    y_test_original = test_data['Global_active_power'].values[TIME_STEPS:]

    # # 5. ç»“æœè¯„ä¼°
    # # 5.1 é€†ç¼©æ”¾
    # dummy_array = np.zeros((len(predictions), X_train.shape[2] - 1))
    # predictions_full = np.concatenate((predictions.reshape(-1,1), dummy_array), axis=1)
    # predictions = scaler.inverse_transform(predictions_full)[:, 0]
    #
    # # è·å–åŸå§‹æµ‹è¯•é›†çš„ Global_active_power
    # y_test_original = test_data['Global_active_power'].values
    #
    # # 5.2 è®¡ç®— MSE å’Œ MAE
    # MSE = mean_squared_error(y_test_original, predictions)
    # MAE = mean_absolute_error(y_test_original, predictions)
    # print(f'{args.model} (Test {args.predict_days}) MSE: {MSE}, MAE: {MAE}')
    # logger.info(f'{args.model} (Test {args.predict_days}) MSE: {MSE}, MAE: {MAE}')
    #
    # # 6. ç»“æœå¯è§†åŒ–
    # plt.figure(figsize=(12, 6))
    # plt.plot(y_test_original, label='Actual')
    # plt.plot(predictions, label=f'{args.model} Predicted')
    # plt.title(f'{args.model} (Test {args.predict_days}): Actual vs Predicted Global Active Power')
    # plt.xlabel('Time Steps')
    # plt.ylabel('Global Active Power')
    # plt.legend()
    #
    # # ä¿å­˜é¢„æµ‹å€¼å’ŒçœŸå®å€¼
    # lstm_results = np.column_stack((y_test_original, predictions))
    # output_file_name = f'{args.model.lower()}_predictions_{args.predict_days}.csv'
    # np.savetxt(os.path.join(output_dir, output_file_name), lstm_results, delimiter=',', header='Actual,Predicted', comments='')
    # plt.savefig(os.path.join(output_dir, f'{args.model.lower()}_predictions_{args.predict_days}.png'))
    # plt.close()

    # -------------------------------
    # 5.1 é€†ç¼©æ”¾ â€”â€” å¾—åˆ° predictionsï¼ˆé¢„æµ‹å€¼ï¼‰
    # -------------------------------
    dummy = np.zeros((len(preds_scaled), X_train.shape[2] - 1))
    pred_full = np.concatenate([preds_scaled.reshape(-1, 1), dummy], axis=1)
    predictions = scaler.inverse_transform(pred_full)[:, 0]  # shape=(N,)

    # -------------------------------
    # 5.2 çœŸå€¼å¯¹é½ â€”â€” å¾—åˆ° y_test_original
    # -------------------------------
    # test_data æ˜¯åŸå§‹æœªå½’ä¸€åŒ–çš„ DataFrame
    # æˆ‘ä»¬å‰é¢ç”¨ sliding_window åˆ æ‰äº†å‰ TIME_STEPS ä¸ªå€¼
    y_test_original = test_data['Global_active_power'].values[TIME_STEPS:]  # shape=(N,)

    # -------------------------------
    # 5.3 è¯„ä¼°æŒ‡æ ‡ â€”â€” ç¡®ä¿ç”¨åˆ°äº† y_test_original
    # -------------------------------
    from sklearn.metrics import mean_squared_error, mean_absolute_error
    MSE = mean_squared_error(y_test_original, predictions)
    MAE = mean_absolute_error(y_test_original, predictions)
    print(f'{args.model} (Test {args.predict_days}) â†’ MSE: {MSE:.4f}, MAE: {MAE:.4f}')
    logger.info(f'{args.model} (Test {args.predict_days}) â†’ MSE: {MSE:.4f}, MAE: {MAE:.4f}')

    # -------------------------------
    # 5.4 ä¿å­˜åˆ° CSV â€”â€” åŒæ—¶ä¿å­˜çœŸå€¼å’Œé¢„æµ‹å€¼
    # -------------------------------

    df_res = pd.DataFrame({
        'Actual': y_test_original,
        'Predicted': predictions
    })
    output_file = os.path.join(output_dir, f'{args.model.lower()}_results_{args.predict_days}.csv')
    df_res.to_csv(output_file, index=False)

    # -------------------------------
    # 6. ç»“æœå¯è§†åŒ– â€”â€” ç”¨ y_test_original ç”»å‡ºçœŸå€¼æ›²çº¿
    # -------------------------------
    import matplotlib.pyplot as plt
    plt.figure(figsize=(12, 6))
    plt.plot(y_test_original, label='Actual')  # çœŸå€¼
    plt.plot(predictions, label=f'{args.model} Predicted')  # é¢„æµ‹
    plt.title(f'{args.model} (Test {args.predict_days}): Actual vs Predicted')
    plt.xlabel('Time Steps')
    plt.ylabel('Global Active Power')
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f'{args.model.lower()}_plot_{args.predict_days}.png'))
    plt.show()


if __name__ == "__main__":
    args = get_parser()
    main(args)
